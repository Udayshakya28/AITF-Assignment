import React, { useState, useEffect, useRef } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import { Mic, MicOff, Volume2 } from 'lucide-react';

const VoiceInput = ({ onVoiceInput, isEnabled = true, language = 'ja' }) => {
  const [isListening, setIsListening] = useState(false);
  const [isSupported, setIsSupported] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [error, setError] = useState(null);
  const recognitionRef = useRef(null);
  const timeoutRef = useRef(null);

  useEffect(() => {
    // Check if Web Speech API is supported
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    
    if (SpeechRecognition) {
      setIsSupported(true);
      
      // Initialize speech recognition
      const recognition = new SpeechRecognition();
      recognition.continuous = false;
      recognition.interimResults = true;
      // Support multiple languages including Chinese
      const languageMap = {
        'ja': 'ja-JP',
        'zh': 'zh-CN',
        'zh-CN': 'zh-CN',
        'zh-TW': 'zh-TW',
        'en': 'en-US'
      };
      recognition.lang = languageMap[language] || 'ja-JP';
      recognition.maxAlternatives = 1;

      recognition.onstart = () => {
        setIsListening(true);
        setError(null);
        setTranscript('');
      };

      recognition.onresult = (event) => {
        let finalTranscript = '';
        let interimTranscript = '';

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }

        setTranscript(finalTranscript || interimTranscript);

        if (finalTranscript) {
          // Clear timeout if we got a final result
          if (timeoutRef.current) {
            clearTimeout(timeoutRef.current);
          }
          
          // Send the final transcript
          onVoiceInput(finalTranscript.trim(), true);
          setTranscript('');
          setIsListening(false);
        }
      };

      recognition.onerror = (event) => {
        console.error('Speech recognition error:', event.error);
        setIsListening(false);
        
        let errorMessage = 'éŸ³å£°èªè­˜ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ';
        switch (event.error) {
          case 'no-speech':
            errorMessage = 'éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ';
            break;
          case 'audio-capture':
            errorMessage = 'ãƒã‚¤ã‚¯ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“';
            break;
          case 'not-allowed':
            errorMessage = 'ãƒã‚¤ã‚¯ã®ä½¿ç”¨ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“';
            break;
          case 'network':
            errorMessage = 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ';
            break;
          case 'language-not-supported':
            errorMessage = 'é¸æŠã•ã‚ŒãŸè¨€èªã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“';
            break;
        }
        
        setError(errorMessage);
        
        // Clear error after 3 seconds
        setTimeout(() => setError(null), 3000);
      };

      recognition.onend = () => {
        setIsListening(false);
        
        // If we have a transcript but recognition ended without final result
        if (transcript && !transcript.trim()) {
          setTranscript('');
        }
      };

      recognitionRef.current = recognition;
    } else {
      setIsSupported(false);
      setError('ãŠä½¿ã„ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°èªè­˜ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“');
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.abort();
      }
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current);
      }
    };
  }, [language, onVoiceInput, transcript]);

  const startListening = () => {
    if (!isSupported || !isEnabled || isListening) return;

    try {
      recognitionRef.current.start();
      
      // Set a timeout to stop listening after 10 seconds
      timeoutRef.current = setTimeout(() => {
        if (recognitionRef.current && isListening) {
          recognitionRef.current.stop();
        }
      }, 10000);
    } catch (error) {
      console.error('Failed to start speech recognition:', error);
      setError('éŸ³å£°èªè­˜ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ');
    }
  };

  const stopListening = () => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop();
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current);
      }
    }
  };

  const toggleListening = () => {
    if (isListening) {
      stopListening();
    } else {
      startListening();
    }
  };

  // Text-to-speech for reading responses (bonus feature)
  const speakText = (text) => {
    if ('speechSynthesis' in window) {
      const utterance = new SpeechSynthesisUtterance(text);
      const languageMap = {
        'ja': 'ja-JP',
        'zh': 'zh-CN',
        'zh-CN': 'zh-CN',
        'zh-TW': 'zh-TW',
        'en': 'en-US'
      };
      utterance.lang = languageMap[language] || 'ja-JP';
      utterance.rate = 0.9;
      utterance.pitch = 1;
      window.speechSynthesis.speak(utterance);
    }
  };

  if (!isSupported) {
    return (
      <div className="text-center text-white/60 font-noto">
        <p className="text-sm">éŸ³å£°å…¥åŠ›ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“</p>
      </div>
    );
  }

  return (
    <div className="flex flex-col items-center space-y-4">
      {/* Voice Input Button */}
      <div className="flex items-center space-x-4">
        <motion.button
          onClick={toggleListening}
          disabled={!isEnabled}
          whileHover={{ scale: 1.05 }}
          whileTap={{ scale: 0.95 }}
          className={`relative p-4 rounded-full transition-all duration-300 ${
            isListening
              ? 'bg-red-500 hover:bg-red-600 recording-pulse'
              : 'bg-blue-500 hover:bg-blue-600'
          } ${
            !isEnabled ? 'opacity-50 cursor-not-allowed' : 'cursor-pointer'
          } text-white shadow-lg`}
        >
          {isListening ? (
            <MicOff className="w-6 h-6" />
          ) : (
            <Mic className="w-6 h-6" />
          )}
          
          {/* Recording indicator */}
          {isListening && (
            <motion.div
              initial={{ scale: 0 }}
              animate={{ scale: 1 }}
              className="absolute -top-1 -right-1 w-3 h-3 bg-red-400 rounded-full"
            />
          )}
        </motion.button>

        {/* Text-to-speech button */}
        <motion.button
          onClick={() => speakText('ã“ã‚“ã«ã¡ã¯ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ')}
          whileHover={{ scale: 1.05 }}
          whileTap={{ scale: 0.95 }}
          className="p-3 bg-purple-500 hover:bg-purple-600 text-white rounded-full
                   transition-colors duration-200 shadow-lg"
          title="éŸ³å£°ã§æŒ¨æ‹¶"
        >
          <Volume2 className="w-5 h-5" />
        </motion.button>
      </div>

      {/* Status and Transcript Display */}
      <AnimatePresence>
        {(isListening || transcript || error) && (
          <motion.div
            initial={{ opacity: 0, y: 10 }}
            animate={{ opacity: 1, y: 0 }}
            exit={{ opacity: 0, y: -10 }}
            className="text-center max-w-md"
          >
            {error ? (
              <div className="bg-red-500/90 text-white px-4 py-2 rounded-lg backdrop-blur-sm">
                <p className="text-sm font-noto">{error}</p>
              </div>
            ) : isListening ? (
              <div className="bg-blue-500/90 text-white px-4 py-2 rounded-lg backdrop-blur-sm">
                <div className="flex items-center justify-center space-x-2">
                  <motion.div
                    animate={{ scale: [1, 1.2, 1] }}
                    transition={{ repeat: Infinity, duration: 1.5 }}
                    className="w-2 h-2 bg-white rounded-full"
                  />
                  <p className="text-sm font-noto">
                    {transcript || 'èã„ã¦ã„ã¾ã™...'}
                  </p>
                </div>
              </div>
            ) : transcript ? (
              <div className="bg-green-500/90 text-white px-4 py-2 rounded-lg backdrop-blur-sm">
                <p className="text-sm font-noto">{transcript}</p>
              </div>
            ) : null}
          </motion.div>
        )}
      </AnimatePresence>

      {/* Instructions */}
      {!isListening && !error && (
        <div className="text-center text-white/60 font-noto">
          <p className="text-sm">
            ğŸ¤ {language === 'zh' || language === 'zh-CN' ? 
              'ç‚¹å‡»éº¦å…‹é£æŒ‰é’®å¼€å§‹è¯­éŸ³è¾“å…¥' : 
              'ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦éŸ³å£°å…¥åŠ›ã‚’é–‹å§‹'}
          </p>
          <p className="text-xs mt-1">
            {language === 'zh' || language === 'zh-CN' ? 
              'æ”¯æŒä¸­æ–‡è¯­éŸ³è¾“å…¥' : 
              'æ—¥æœ¬èªã§ã®éŸ³å£°å…¥åŠ›ã«å¯¾å¿œã—ã¦ã„ã¾ã™'}
          </p>
        </div>
      )}
    </div>
  );
};

export default VoiceInput;

